{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b36b560d",
   "metadata": {},
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tabular_benchmark.utils.misc import set_seeds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c455754a",
   "metadata": {},
   "source": [
    "# Comparision between GBDT model from sklearn and tabular_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c160709",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e052f",
   "metadata": {},
   "source": [
    "### Load and Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775e387c",
   "metadata": {},
   "source": [
    "This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "For more information:\n",
    "https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "618f9a12",
   "metadata": {},
   "source": [
    "dataset = datasets.fetch_california_housing()\n",
    "dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a34d8f42",
   "metadata": {},
   "source": [
    "8 numeric, predictive attributes and the target\n",
    "- MedInc median income in block group\n",
    "- HouseAge median house age in block group\n",
    "- AveRooms average number of rooms per household\n",
    "- AveBedrms average number of bedrooms per household\n",
    "- Population block group population\n",
    "- AveOccup average number of household members\n",
    "- Latitude block group latitude\n",
    "- Longitude block group longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f924dfd",
   "metadata": {},
   "source": [
    "raw_X = dataset['data']\n",
    "raw_X"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a85f824c",
   "metadata": {},
   "source": [
    "20640 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8217c0d",
   "metadata": {},
   "source": [
    "raw_X.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3ab6d434",
   "metadata": {},
   "source": [
    "The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000). Therefore, we are dealing with a **regression** problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b2ae56d",
   "metadata": {},
   "source": [
    "raw_y = dataset['target']\n",
    "raw_y "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21631a72",
   "metadata": {},
   "source": [
    "plt.hist(raw_y)\n",
    "plt.xlabel('House Value (thousands of USD)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('california_housing_dist')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "50ba224b",
   "metadata": {},
   "source": [
    "### Load default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b70bfa",
   "metadata": {},
   "source": [
    "model = GradientBoostingRegressor()  # classic GBDT from sklearn for regression"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "136f6f58",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c082415",
   "metadata": {},
   "source": [
    "We need to do at least some preprocessing, starting by spliting the data in train, validation and test. We will also transform our data to a pandas dataframe to facilitate our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c58402f",
   "metadata": {},
   "source": [
    "set_seeds(42)  # set seeds of python, numpy and pytorch altogheter\n",
    "raw_X = pd.DataFrame(raw_X)\n",
    "raw_y = pd.DataFrame(raw_y)\n",
    "pct_test = 0.2  # pct of total data\n",
    "pct_valid = 0.1 * (1 - pct_test)  # pct of total data\n",
    "seed = 42  # reproducibility\n",
    "train_data, X_test, train_target, y_test = train_test_split(raw_X, raw_y, test_size=pct_test, random_state=seed)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_target, test_size=pct_valid, random_state=seed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe350c75",
   "metadata": {},
   "source": [
    "X_train"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1ee654fa",
   "metadata": {},
   "source": [
    "We need to ensure that there is no missing data. We will replace any missing data by the feature median. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acdf63ce",
   "metadata": {},
   "source": [
    "median_nan = X_train.median()\n",
    "X_train = X_train.fillna(median_nan)\n",
    "X_valid = X_valid.fillna(median_nan)\n",
    "X_test = X_test.fillna(median_nan)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6e029150",
   "metadata": {},
   "source": [
    "We know that we dot not have any categorical data, if we had, we would also need to ensure that there is no missing data (replacing missing values for the mode of the feature, for example) and we would need to encode the categorical features using some strategy (ordinal encoding, one-hot encoding etc).\n",
    "\n",
    "We will now cast our data to a standard type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0985a9a",
   "metadata": {},
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_valid = X_valid.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_valid = y_valid.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "365a7fd1",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab65185f",
   "metadata": {},
   "source": [
    "y_train = np.ravel(y_train)  # sklearn expects the shape of y to be (n_samples, ) \n",
    "model.fit(X_train, y_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4015e0d8",
   "metadata": {},
   "source": [
    "### Visualizing the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cbc1aa",
   "metadata": {},
   "source": [
    "By default the model uses the squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a660b793",
   "metadata": {},
   "source": [
    "model.train_score_"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c18d676",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(model.train_score_)\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('MSE')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36ef7010",
   "metadata": {},
   "source": [
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "results_columns = ['Model', 'RMSE']\n",
    "results = pd.DataFrame(columns=results_columns)\n",
    "res = ['Scikit-learn GBDT', rmse]\n",
    "res_row = pd.DataFrame(dict(zip(results_columns, res)), index=[0])\n",
    "results = pd.concat([results, res_row], ignore_index=True)\n",
    "results"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "42bd750d",
   "metadata": {},
   "source": [
    "## Tabular benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98b06059",
   "metadata": {},
   "source": [
    "from tabular_benchmark.utils.datasets import load_dataset\n",
    "from tabular_benchmark.models.xgboost import XGBoostModel\n",
    "from tabular_benchmark.utils.misc import train_valid_test_split"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0710b551",
   "metadata": {},
   "source": [
    "### Load and Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f1083f8",
   "metadata": {},
   "source": [
    "dataset = load_dataset('california_housing')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a03c11a0",
   "metadata": {},
   "source": [
    "raw_X = dataset['data']\n",
    "raw_X"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9c7cd2d",
   "metadata": {},
   "source": [
    "raw_y = dataset['target']\n",
    "raw_y"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7724d8cf",
   "metadata": {},
   "source": [
    "Note that we are already working with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7e38237",
   "metadata": {},
   "source": [
    "task = dataset['task']\n",
    "cat_features = dataset['cat_features']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "990c96d5",
   "metadata": {},
   "source": [
    "set_seeds(42)  # set seeds of python, numpy and pytorch altogheter\n",
    "pct_test = 0.2  # pct of total data\n",
    "pct_valid = 0.1 * (1 - pct_test)  # pct of total data\n",
    "seed = 42  # reproducibility\n",
    "train_data, X_test, train_target, y_test = train_test_split(raw_X, raw_y, test_size=pct_test, random_state=seed)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_target, test_size=pct_valid, random_state=seed)\n",
    "train = [X_train, y_train]\n",
    "valid = [X_valid, y_valid]\n",
    "test = [X_test, y_test]\n",
    "# We could run the code below, but we are using the same split as before to compare the methods\n",
    "# pct_test = 0.2  # pct of total data\n",
    "# pct_valid = 0.1 * (1 - pct_test)  # pct of total data\n",
    "# train, valid, test = train_valid_test_split(raw_X, raw_y, pct_valid, pct_test, random_state=seed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "658ece2e",
   "metadata": {},
   "source": [
    "Each set is a list of [features, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27fd2da0",
   "metadata": {},
   "source": [
    "train"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d3de24db",
   "metadata": {},
   "source": [
    "### Load default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a851ddd7",
   "metadata": {},
   "source": [
    "model = XGBoostModel()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "58823859",
   "metadata": {},
   "source": [
    "Note that we do not need to specify if it is a regressor or a classifier, we will automatically instanciate the right model given the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0642c",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b2ffef0",
   "metadata": {},
   "source": [
    "train, valid, test, p_cat_features, p_cat_dims = model.preprocess_dataset(train=train, task=task, cat_features=cat_features,\n",
    "                                                                          valid=valid, test=test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f179b561",
   "metadata": {},
   "source": [
    "This function automatically treat missing values, encode categorical features, cast the variables to its correct type and normalize the features following the recommendations for each model. It also return the categorical features and the dimensions of each categorical feature, which are needed for some models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b15ada",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8af35c6c",
   "metadata": {},
   "source": [
    "X_train = train[0]\n",
    "y_train = train[1]\n",
    "eval_sets = [train, valid]\n",
    "eval_names = ['train', 'valid']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c723d8b",
   "metadata": {},
   "source": [
    "model.fit(X_train=X_train, y_train=y_train, task=task, eval_sets=eval_sets, eval_names=eval_names, \n",
    "          cat_features=p_cat_features, cat_dims=p_cat_dims)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c49a5322",
   "metadata": {},
   "source": [
    "### Visualizing the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add072e",
   "metadata": {},
   "source": [
    "By default the model uses **root** mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c7314f2",
   "metadata": {},
   "source": [
    "model.evals_result_"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d833a2f5",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(model.evals_result_['train']['loss_rmse'], label='train')\n",
    "ax.plot(model.evals_result_['valid']['loss_rmse'], label='valid')\n",
    "ax.legend()\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('RMSE')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "caeb706c",
   "metadata": {},
   "source": [
    "Note that we use by default the valid set for early stopping (we could do the same with sklearn with some tweaking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b60e96e",
   "metadata": {},
   "source": [
    "y_pred = model.predict(test[0])\n",
    "rmse = mean_squared_error(test[1], y_pred, squared=False)\n",
    "res = ['XGboost', rmse]\n",
    "res_row = pd.DataFrame(dict(zip(results_columns, res)), index=[0])\n",
    "results = pd.concat([results, res_row], ignore_index=True)\n",
    "results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97e6400b",
   "metadata": {},
   "source": [
    "# Or Equivalently...\n",
    "model.evaluate_set(test, 'rmse')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd6c484",
   "metadata": {},
   "source": [
    "### Beyond the default model...tunning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dae015fa",
   "metadata": {},
   "source": [
    "from tabular_benchmark.tunners import DEHBTunner"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d482a47a",
   "metadata": {},
   "source": [
    "model = XGBoostModel()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "25dacd56",
   "metadata": {},
   "source": [
    "We use the total training data, before spliting into the validation set, and we choose a budget_type ('iteration' or 'subsample') to perform the tunning of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0481ded0",
   "metadata": {},
   "source": [
    "tunner = DEHBTunner(budget_type='iteration',\n",
    "                    min_budget=100, max_budget=10000, resample_strategy='k-fold_cv', k_folds=5,\n",
    "                    seed_dataset=seed, seed_model=seed,\n",
    "                    n_workers=5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "11394e70",
   "metadata": {},
   "source": [
    "We run until one of the conditions is met (we should only set **one**):\n",
    "- we evaluate a certain amount of models (fevals)\n",
    "- we evaluate a certain amount of brackets (min budget to max budget)\n",
    "- we evaluate for a certain amount of time in seconds (total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b3b8c42",
   "metadata": {},
   "source": [
    "fevals = None\n",
    "brackets = 10\n",
    "total_cost = None\n",
    "trajectory, runtime, history = tunner.run(model.__class__, train_data, train_target, task, cat_features,\n",
    "                                          fevals=fevals, brackets=brackets, total_cost=total_cost,\n",
    "                                          verbose=False, save_intermediate=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd22b9b",
   "metadata": {},
   "source": [
    "tunner.tunner.get_incumbents()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320d2d2",
   "metadata": {},
   "source": [
    "model = XGBoostModel(**tunner.tunner.get_incumbents()[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335e722",
   "metadata": {},
   "source": [
    "model.fit(X_train=X_train, y_train=y_train, task=task, eval_sets=eval_sets, eval_names=eval_names, \n",
    "          cat_features=p_cat_features, cat_dims=p_cat_dims)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc77643",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(model.evals_result_['train']['rmse'], label='train')\n",
    "ax.plot(model.evals_result_['valid']['rmse'], label='valid')\n",
    "ax.legend()\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('RMSE')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df7509",
   "metadata": {},
   "source": [
    "y_pred = model.predict(test[0])\n",
    "rmse = mean_squared_error(test[1], y_pred, squared=False)\n",
    "res = ['XGboost Tunned', rmse]\n",
    "res_row = pd.DataFrame(dict(zip(results_columns, res)), index=[0])\n",
    "results = pd.concat([results, res_row], ignore_index=True)\n",
    "results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caaf962",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tabular]",
   "language": "python",
   "name": "conda-env-tabular-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
