{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b36b560d",
   "metadata": {},
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from tabular_benchmark.utils.misc import set_seeds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c455754a",
   "metadata": {},
   "source": [
    "# Comparision between GBDT model from sklearn and tabular_benchmark for classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c160709",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e052f",
   "metadata": {},
   "source": [
    "### Load and Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775e387c",
   "metadata": {},
   "source": [
    "This file concerns credit card applications. All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data.\n",
    "\n",
    "This dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values. There are also a few missing values.\n",
    "\n",
    "For more information:\n",
    "https://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6da859",
   "metadata": {},
   "source": [
    "dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat', \n",
    "                      sep='\\s+', header=None)\n",
    "dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a34d8f42",
   "metadata": {},
   "source": [
    "There are 6 numerical and 8 categorical attributes. The labels have been changed for the convenience of the statistical algorithms. For example, attribute 4 originally had 3 labels p,g,gg and these have been changed to labels 1,2,3.\n",
    "\n",
    "- A1: 0,1 CATEGORICAL (formerly: a,b)\n",
    "- A2: continuous.\n",
    "- A3: continuous.\n",
    "- A4: 1,2,3 CATEGORICAL (formerly: p,g,gg)\n",
    "- A5: 1, 2,3,4,5, 6,7,8,9,10,11,12,13,14 CATEGORICAL (formerly: ff,d,i,k,j,aa,m,c,w, e, q, r,cc, x)\n",
    "- A6: 1, 2,3, 4,5,6,7,8,9 CATEGORICAL (formerly: ff,dd,j,bb,v,n,o,h,z)\n",
    "- A7: continuous.\n",
    "- A8: 1, 0 CATEGORICAL (formerly: t, f)\n",
    "- A9: 1, 0 CATEGORICAL (formerly: t, f)\n",
    "- A10: continuous.\n",
    "- A11: 1, 0 CATEGORICAL (formerly t, f)\n",
    "- A12: 1, 2, 3 CATEGORICAL (formerly: s, g, p)\n",
    "- A13: continuous.\n",
    "- A14: continuous.\n",
    "- A15: 1,2 class attribute (formerly: +,-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a53618",
   "metadata": {},
   "source": [
    "cat_features = [0, 3, 4, 5, 7, 8, 10, 11]\n",
    "target = 14"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f924dfd",
   "metadata": {},
   "source": [
    "raw_X = dataset.drop(target, axis=1)\n",
    "raw_X"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a85f824c",
   "metadata": {},
   "source": [
    "690 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab6d434",
   "metadata": {},
   "source": [
    "The target variable is the approval or refusal of the credit card application. Therefore, we are dealing with a **classification** problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b2ae56d",
   "metadata": {},
   "source": [
    "raw_y = pd.DataFrame(dataset[target])\n",
    "raw_y = raw_y.rename(columns={14: 'Approved?'})\n",
    "raw_y "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21631a72",
   "metadata": {},
   "source": [
    "plt.hist(raw_y)\n",
    "plt.savefig('australian_credit_dist')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "50ba224b",
   "metadata": {},
   "source": [
    "### Load default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b70bfa",
   "metadata": {},
   "source": [
    "model = GradientBoostingClassifier()  # classic GBDT from sklearn for classification"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "136f6f58",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c082415",
   "metadata": {},
   "source": [
    "We need to do at least some preprocessing, starting by spliting the data in train, validation and test. The data is already in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c58402f",
   "metadata": {},
   "source": [
    "seed = 42\n",
    "set_seeds(seed)  # set seeds of python, numpy and pytorch altogheter\n",
    "pct_test = 0.2  # pct of total data\n",
    "pct_valid = 0.1 * (1 - pct_test)  # pct of total data\n",
    "train_data, X_test, train_target, y_test = train_test_split(raw_X, raw_y, test_size=pct_test)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_target, test_size=pct_valid)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe350c75",
   "metadata": {},
   "source": [
    "X_train"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e613ec5",
   "metadata": {},
   "source": [
    "y_train"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1ee654fa",
   "metadata": {},
   "source": [
    "We need to ensure that there is no missing data. We will replace any missing data by the feature median for continuous data and by the feature mode for categorical data. (Note that for this particular dataset they have already replaced the missing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acdf63ce",
   "metadata": {},
   "source": [
    "# split categirucal and continuous data\n",
    "all_features = [i for i in range(X_train.shape[1])]\n",
    "cont_features = list(set(all_features) - set(cat_features))\n",
    "data_cat = X_train.iloc[:, cat_features]\n",
    "data_cont = X_train.iloc[:, cont_features]\n",
    "# preprocess categorical\n",
    "mode_nan = data_cat.mode().iloc[0, :]\n",
    "data_cat = data_cat.fillna(mode_nan)\n",
    "# preprocess continuous\n",
    "median_nan = data_cont.median()\n",
    "data_cont = data_cont.fillna(median_nan)\n",
    "# encode target\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.ravel(y_train))\n",
    "y = pd.DataFrame(label_encoder.transform(np.ravel(y_train)), columns=y_train.columns)\n",
    "# encode categoiral data\n",
    "cat_dims = data_cat.nunique(dropna=False).to_list()\n",
    "unknown_value = max(cat_dims)\n",
    "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=unknown_value)\n",
    "encoder.fit(X=data_cat, y=y)\n",
    "data_cat = pd.DataFrame(encoder.transform(data_cat), index=data_cat.index)\n",
    "# cast data\n",
    "data_cont = data_cont.astype(np.float32)\n",
    "data_cat = data_cat.astype(np.float32)\n",
    "# join data\n",
    "data = pd.concat([data_cont, data_cat.set_index(data_cont.index)], axis=1).set_axis(cont_features + cat_features, axis=1)\n",
    "target = y\n",
    "train_preprocessed = [data, target]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f59f9198",
   "metadata": {},
   "source": [
    "# Do the same for validation...(without refiting encoders, mode and medians)\n",
    "# split categirucal and continuous data\n",
    "data_cat = X_valid.iloc[:, cat_features]\n",
    "data_cont = X_valid.iloc[:, cont_features]\n",
    "# preprocess categorical\n",
    "data_cat = data_cat.fillna(mode_nan)\n",
    "# preprocess continuous\n",
    "data_cont = data_cont.fillna(median_nan)\n",
    "# encode target\n",
    "y = pd.DataFrame(label_encoder.transform(np.ravel(y_valid)), columns=y_valid.columns)\n",
    "# encode categoiral data\n",
    "data_cat = pd.DataFrame(encoder.transform(data_cat), index=data_cat.index)\n",
    "# cast data\n",
    "data_cont = data_cont.astype(np.float32)\n",
    "data_cat = data_cat.astype(np.float32)\n",
    "# join data\n",
    "data = pd.concat([data_cont, data_cat.set_index(data_cont.index)], axis=1).set_axis(cont_features + cat_features, axis=1)\n",
    "target = y\n",
    "valid_preprocessed = [data, target]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "025ecb16",
   "metadata": {},
   "source": [
    "# Do the same for test...(without refiting encoders, mode and medians)\n",
    "# split categirucal and continuous data\n",
    "data_cat = X_test.iloc[:, cat_features]\n",
    "data_cont = X_test.iloc[:, cont_features]\n",
    "# preprocess categorical\n",
    "data_cat = data_cat.fillna(mode_nan)\n",
    "# preprocess continuous\n",
    "data_cont = data_cont.fillna(median_nan)\n",
    "# encode target\n",
    "y = pd.DataFrame(label_encoder.transform(np.ravel(y_test)), columns=y_test.columns)\n",
    "# encode categoiral data\n",
    "data_cat = pd.DataFrame(encoder.transform(data_cat), index=data_cat.index)\n",
    "# cast data\n",
    "data_cont = data_cont.astype(np.float32)\n",
    "data_cat = data_cat.astype(np.float32)\n",
    "# join data\n",
    "data = pd.concat([data_cont, data_cat.set_index(data_cont.index)], axis=1).set_axis(cont_features + cat_features, axis=1)\n",
    "target = y\n",
    "test_preprocessed = [data, target]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "365a7fd1",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab65185f",
   "metadata": {},
   "source": [
    "model.fit(train_preprocessed[0], np.ravel(train_preprocessed[1]))  # sklearn expects the shape of y to be (n_samples, ) "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4015e0d8",
   "metadata": {},
   "source": [
    "### Visualizing the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cbc1aa",
   "metadata": {},
   "source": [
    "By default the model uses the log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a660b793",
   "metadata": {},
   "source": [
    "model.train_score_"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c18d676",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(model.train_score_)\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Logloss')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36ef7010",
   "metadata": {},
   "source": [
    "results_columns = ['Model', 'LogLoss', 'Accuracy']\n",
    "results = pd.DataFrame(columns=results_columns)\n",
    "y_pred = model.predict(test_preprocessed[0])\n",
    "acc = accuracy_score(test_preprocessed[1], y_pred)\n",
    "y_pred_proba = model.predict_proba(test_preprocessed[0])\n",
    "logloss = log_loss(test_preprocessed[1], y_pred_proba)\n",
    "res = ['Scikit-learn GBDT', logloss, acc]\n",
    "res_row = pd.DataFrame(dict(zip(results_columns, res)), index=[0])\n",
    "results = pd.concat([results, res_row], ignore_index=True)\n",
    "results"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "42bd750d",
   "metadata": {},
   "source": [
    "## Tabular benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98b06059",
   "metadata": {},
   "source": [
    "from tabular_benchmark.utils.datasets import load_dataset\n",
    "from tabular_benchmark.models.xgboost import XGBoostModel\n",
    "from tabular_benchmark.utils.misc import train_valid_test_split"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0710b551",
   "metadata": {},
   "source": [
    "### Load and Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc319c",
   "metadata": {},
   "source": [
    "We can directly load the dataset using the load_dataset function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f1083f8",
   "metadata": {},
   "source": [
    "dataset = load_dataset('australian_credit')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dddefc85",
   "metadata": {},
   "source": [
    "dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8d747094",
   "metadata": {},
   "source": [
    "For other datasets, we can construct a similar dictionary as the retuned by load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27b094cb",
   "metadata": {},
   "source": [
    "name = 'australian_credit'\n",
    "task = 'classification'\n",
    "full_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat', \n",
    "                        sep='\\s+', header=None)\n",
    "data = full_data.iloc[:, :-1]\n",
    "target = pd.DataFrame(full_data.iloc[:, 14])\n",
    "test_data = None\n",
    "test_target = None\n",
    "cat_features = [0, 3, 4, 5, 7, 8, 10, 11]\n",
    "dataset = {\n",
    "    'name': name,\n",
    "    'task': task,\n",
    "    'data': data,\n",
    "    'target': target,\n",
    "    'test_data': test_data,\n",
    "    'test_target': test_target,\n",
    "    'cat_features': cat_features,\n",
    "}\n",
    "dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a03c11a0",
   "metadata": {},
   "source": [
    "raw_X = dataset['data']\n",
    "raw_X"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9c7cd2d",
   "metadata": {},
   "source": [
    "raw_y = dataset['target']\n",
    "raw_y"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7724d8cf",
   "metadata": {},
   "source": [
    "Note that we are already working with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7e38237",
   "metadata": {},
   "source": [
    "task = dataset['task']\n",
    "cat_features = dataset['cat_features']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "990c96d5",
   "metadata": {},
   "source": [
    "set_seeds(seed)\n",
    "pct_test = 0.2  # pct of total data\n",
    "pct_valid = 0.1 * (1 - pct_test)  # pct of total data\n",
    "train_data, X_test, train_target, y_test = train_test_split(raw_X, raw_y, test_size=pct_test)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_target, test_size=pct_valid)\n",
    "train = [X_train, y_train]\n",
    "valid = [X_valid, y_valid]\n",
    "test = [X_test, y_test]\n",
    "# We could run the code below, but we are using the same split as before to compare the methods\n",
    "# pct_test = 0.2  # pct of total data\n",
    "# pct_valid = 0.1 * (1 - pct_test)  # pct of total data\n",
    "# train, valid, test = train_valid_test_split(raw_X, raw_y, pct_valid, pct_test, random_state=seed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "658ece2e",
   "metadata": {},
   "source": [
    "Each set is a list of [features, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27fd2da0",
   "metadata": {},
   "source": [
    "train"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d3de24db",
   "metadata": {},
   "source": [
    "### Load default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a851ddd7",
   "metadata": {},
   "source": [
    "model = XGBoostModel(random_seed=seed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "58823859",
   "metadata": {},
   "source": [
    "Note that we do not need to specify if it is a regressor or a classifier, we will automatically instanciate the right model given the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0642c",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b2ffef0",
   "metadata": {},
   "source": [
    "train, valid, test, p_cat_features, p_cat_dims = model.preprocess_dataset(train=train, task=task, cat_features=cat_features,\n",
    "                                                                          valid=valid, test=test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f179b561",
   "metadata": {},
   "source": [
    "This function automatically treat missing values, encode categorical features, cast the variables to its correct type and normalize the features following the recommendations for each model. It also return the categorical features and the dimensions of each categorical feature, which are needed for some models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b15ada",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8af35c6c",
   "metadata": {},
   "source": [
    "X_train = train[0]\n",
    "y_train = train[1]\n",
    "eval_sets = [train, valid]\n",
    "eval_names = ['train', 'valid']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c723d8b",
   "metadata": {},
   "source": [
    "model.fit(X_train=X_train, y_train=y_train, task=task, eval_sets=eval_sets, eval_names=eval_names, \n",
    "          cat_features=p_cat_features, cat_dims=p_cat_dims)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c49a5322",
   "metadata": {},
   "source": [
    "### Visualizing the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add072e",
   "metadata": {},
   "source": [
    "By default the model uses log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c7314f2",
   "metadata": {},
   "source": [
    "model.evals_result_"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d833a2f5",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(model.evals_result_['train']['loss_logloss'], label='train')\n",
    "ax.plot(model.evals_result_['valid']['loss_logloss'], label='valid')\n",
    "ax.legend()\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('logloss')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "caeb706c",
   "metadata": {},
   "source": [
    "Note that we use by default the valid set for early stopping (we could do the same with sklearn with some tweaking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b60e96e",
   "metadata": {},
   "source": [
    "y_pred = model.predict(test[0])\n",
    "acc = accuracy_score(test[1], y_pred)\n",
    "y_pred_proba = model.predict_proba(test[0])\n",
    "logloss = log_loss(test[1], y_pred_proba)\n",
    "res = ['XGBoost', logloss, acc]\n",
    "res_row = pd.DataFrame(dict(zip(results_columns, res)), index=[0])\n",
    "results = pd.concat([results, res_row], ignore_index=True)\n",
    "results"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd6c484",
   "metadata": {},
   "source": [
    "### Beyond the default model...tunning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dae015fa",
   "metadata": {},
   "source": [
    "from tabular_benchmark.tunners import DEHBTunner"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d482a47a",
   "metadata": {},
   "source": [
    "model = XGBoostModel(random_seed=seed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "25dacd56",
   "metadata": {},
   "source": [
    "We use the total training data, before spliting into the validation set, and we choose a budget_type ('iteration' or 'subsample') to perform the tunning of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0481ded0",
   "metadata": {},
   "source": [
    "tunner = DEHBTunner(budget_type='iteration',\n",
    "                    min_budget=100, max_budget=10000, resample_strategy='k-fold_cv', k_folds=5,\n",
    "                    n_workers=1, seed_model=seed, seed_dataset=seed)  \n",
    "# note that we can use more than 1 worker, but the results will not be reproductible due to assyncronous nature of the \n",
    "# algorithm in its parallel version"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "11394e70",
   "metadata": {},
   "source": [
    "We run until one of the conditions is met (we should only set **one**):\n",
    "- we evaluate a certain amount of models (fevals)\n",
    "- we evaluate a certain amount of brackets (min budget to max budget)\n",
    "- we evaluate for a certain amount of time in seconds (total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b3b8c42",
   "metadata": {},
   "source": [
    "fevals = None\n",
    "brackets = 10\n",
    "total_cost = None\n",
    "trajectory, runtime, history = tunner.run(model.__class__, train_data, train_target, task, cat_features,\n",
    "                                          fevals=fevals, brackets=brackets, total_cost=total_cost,\n",
    "                                          verbose=False, save_intermediate=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fd22b9b",
   "metadata": {},
   "source": [
    "tunner.tunner.get_incumbents()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9320d2d2",
   "metadata": {},
   "source": [
    "model = XGBoostModel(**tunner.tunner.get_incumbents()[0], random_seed=seed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f335e722",
   "metadata": {},
   "source": [
    "model.fit(X_train=X_train, y_train=y_train, task=task, eval_sets=eval_sets, eval_names=eval_names, \n",
    "          cat_features=p_cat_features, cat_dims=p_cat_dims)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fcc77643",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(model.evals_result_['train']['loss_logloss'], label='train')\n",
    "ax.plot(model.evals_result_['valid']['loss_logloss'], label='valid')\n",
    "ax.legend()\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('logloss')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52df7509",
   "metadata": {},
   "source": [
    "y_pred = model.predict(test[0])\n",
    "acc = accuracy_score(test[1], y_pred)\n",
    "y_pred_proba = model.predict_proba(test[0])\n",
    "logloss = log_loss(test[1], y_pred_proba)\n",
    "res = ['XGBoost Tunned', logloss, acc]\n",
    "res_row = pd.DataFrame(dict(zip(results_columns, res)), index=[0])\n",
    "results = pd.concat([results, res_row], ignore_index=True)\n",
    "results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0442a70",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tabular]",
   "language": "python",
   "name": "conda-env-tabular-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
